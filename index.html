<!DOCTYPE HTML>
<html>
	<head>
		<title>Te-Yen Wu</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<meta name="description" content="Te-Yen Wu is a PhD candidate at Dartmouth College">
		<!-- <base href="https://teyenwu.com/"> -->
		
		<!-- <link rel="stylesheet" href="assets/css/main.css" /> -->
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.7/css/all.min.css">
		<link rel="stylesheet" href="assets/css/custom-style.css" />
		<meta charset="utf-8">
		<!-- Google tag (gtag.js) -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-1LXFW6JX1J"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'G-1LXFW6JX1J');
		</script>
	</head>
	<body class="is-preload">
		 <nav class="navbar fixed-top navbar-expand-lg navbar-light" style="background-color: #FFFFFF; ">
		  <a class="navbar-brand mb-0 h1" href="">Te-Yen Wu</a>
		  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarText" aria-controls="navbarText" aria-expanded="false" aria-label="Toggle navigation">
		    <span class="navbar-toggler-icon"></span>
		  </button>
		  <div class="collapse navbar-collapse" id="navbarText">
		    <ul class="navbar-nav ml-auto">
		      <li class="nav-item active">
		        <a class="nav-link" href="#">Home <span class="sr-only">(current)</span></a>
		      </li>
		      <li class="nav-item">
		        <a class="nav-link" href="publications/TeYenWu.pdf">CV/Resume</a>
		      </li>
		      <li class="nav-item">
		        <a class="nav-link" href="https://scholar.google.com/citations?hl=en&user=TMrlaiUAAAAJ">Google Scholar</a>
		      </li>
		      <li class="nav-item">
		        <a class="nav-link" href="#publications">Publications</a>
		      </li>
		      <li class="nav-item">
		        <a class="nav-link" href="mailto:te-yen.wu.gr@dartmouth.edu">Email</a>
		      </li>
		    </ul>
		  </div>
		</nav>
		<div class="container" style="margin-top: 100px">
            <div class="row flex-row-reverse">
                <div class="col-8 col-12-medium">
                    <div class="about-text go-to text-justify">
                        <h3 class="dark-color">Te-Yen Wu</h3>
                        <h6 class="theme-color lead">HCI researcher, Fullstack electrical engineer and computer scientist</h6>
	                    <p> Te-Yen Wu is a PhD candidate from <a href="https://www.cs.dartmouth.edu/~hci/">XDiscovery Lab</a> at  <a href="https://home.dartmouth.edu"> Dartmouth College</a>, advised by <a href="https://www.cs.dartmouth.edu/~xingdong/"> Xing-Dong Yang</a>.
									His research vision is to create smart environments through smart everyday materials that can be easily and seamlessly integrated into everyday objects at large scale using established methods (e.g. cutting and assembly).
									Towards the vision, his PhD research focuses on <strong>Human-Material Interactions</strong> (HMI), which involves the design, development, and evaluation of interactions and new interfaces of smart materials. 
									In addition to HMI, he has also worked on other topics, such as <strong> wearable input system</strong>, <strong> AR/VR interactions</strong>, <strong>accessible interface</strong> and <strong>educational Tool</strong>. He has a strong record of 18/20 papers published at top HCI venues (CHI, and UIST). This includes a Best paper award at UIST 19 and an Honorable Mention award at CHI 2020. His research has also attracted numerous public interests via Internet News (e.g. Engadget, Times).   <strong>He is active on the job market this year (2022)</strong>. Please feel free to reach out to him for any opportunity. 
						</p>
                    </div>
                </div>
                <div class="col-4 col-12-medium">
                    <div class="about-avatar">
                        <span class="image fit"><img src="images/TeYenWu.png" alt="" /></span>
                    </div>
                </div>
            </div>
        </div>
		<div class="container" style="margin-top: 50px; margin-bottom: 50px">
			 <h2 >Research Overview</h2>
             <div class="row taxonomy" style="margin-top:20px;">
	            <div class="col-lg-6 mb-6">
	                <img src="images/FutureMaterials.jpg">
	            </div>
				
				<div style="margin-top:0px;" class="col-lg-6 mb-8 text-justify"><p>The goal of Ubiquitous Computing is to seamlessly integrate computers into everyday objects, to create a smart environment that can predictively understand and fulfill usersâ€™ needs. However, today technologies are leading to a bottleneck, as most hardware components (e.g., touchpads) are not easily compatible with everyday objects (e.g., garments) and too technical to use (e.g., knowledge about circuit design). To overcome this barrier, my research vision is to create a smart environment using smart everyday materials that can be seamlessly integrated into daily objects at large scale using established methods (e.g. cutting and assembly). To achieve this vision, I studied human-material interactions (HMI) to understand how smart materials should be made to empower people to create smart environments. For more details, please read my <u><strong><a href="publications/ResearchStatement.pdf">research statement.</a> </strong></u></p> </div>

				
        	</div>
       </div>
       <div class="container" style="margin-top: 50px; margin-bottom: 50px">
	       <div class="row">
	            <!-- left column -->
	            <div class="col-lg-8 mb-2">
	                <h2>
	                    Selected Publications
	                </h2>
	                <br/>

	                <div class="publications" id="publications">
		                <h5>
	                    Smart Materials
		                </h5>
						<div class="line"></div>
		                <div class="row research-project" data-sort="2022-9-7">
		                    <div class="col-md-4">
		                       <a href="publications/iWood.pdf" class="image fit"><img src="images/iWood.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									iWood: Makeable Vibration Sensor for Interactive Plywood
		                        </h6>
		                         <p class="text-muted">
		                            <strong>Te-Yen Wu </strong>, Xing-Dong Yang. (UIST 2022)<br>

									<a class="info" href="">[Video]</a>
		                            <a class="info" href="">[DOI]</a>
		                            <a class="info" href="publications/iWood.pdf">[PDF]</a>
		                        </p>
		                          <p>
									iWood is interactive plywood that can sense vibration based on triboelectric effect. As a material, iWood survives common woodworking operations, such as sawing, screwing, and nailing and can be used to create furniture and artifacts.
		                        </p>
		                      
		                    </div>
		                </div>
		               
						<div class="row research-project" data-sort="2022-5-6">
		                    <div class="col-md-4">
		                       <a href="publications/NFCe.pdf" class="image fit"><img src="images/NFCe.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									Body-Centric NFC: Body-Centric Interaction with NFC Devices Through Near-Field Enabled Clothing
		                        </h6>
		                        <p class="text-muted">
		                            <strong>Te-Yen Wu </strong>, Huizhong Ye, Chi-Jung Lee, Xing-Dong Yang, Bing-Yu Chen, Rong-Hao Liang(DIS 2022)<br>

									<a class="info" href="https://vimeo.com/725012677">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/10.1145/3532106.3534569">[DOI]</a>
		                            <a class="info" href="publications/NFCe.pdf">[PDF]</a>
		                        </p>
		                          <p>
									  This paper presents an investigation of body-centric interactions between the NFC device users and their surroundings, and an accessible method for fabricating fexible, extensible, and scalable NFC extenders on clothing pieces, and an easy-to-use toolkit for facilitating designers to realize the interactive experiences.
		                        </p>
		                    </div>
		                </div>
		                <div class="row research-project" data-sort="2021-1-6">
		                    <div class="col-md-4">
		                       <a href="https://www.microsoft.com/en-us/research/publication/project-tasca-enabling-touch-and-contextual-interactions-with-a-pocket-based-textile-sensor/" class="image fit"><img src="images/Tasca.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									Project Tasca: Enabling Touch and Contextual Interactions with a Pocket-based Textile Sensor
		                        </h6>
		                        <p class="text-muted">
		                            <strong>Te-Yen Wu </strong>, Zheer Xu, Xing-Dong Yang, Steve Hodges, Teddy Seyed(CHI 2021)<br>

									<a class="info" href="https://www.youtube.com/watch?v=SsLtLPfw_LE">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/10.1145/3411764.3445712">[DOI]</a>
		                            <a class="info" href="https://www.microsoft.com/en-us/research/publication/project-tasca-enabling-touch-and-contextual-interactions-with-a-pocket-based-textile-sensor/">[PDF]</a>
		                        </p>
		                       
		                          <p>
									   Project Tasca presents a pocket-based textile sensor that detects user input and recognizes everyday objects usually carried in the pockets of a pair of pants (e.g., keys, coins, electronic devices, or plastic items). By creating a new fabric-based sensor capable of detecting in-pocket touch and pressure, and recognizing metallic, non-metallic, and tagged objects inside the pocket, we enable a rich variety of subtle, eyes-free, and always-available input, as well as context-driven interactions in wearable scenarios
		                        </p>

		                    </div>
		                </div>
		                <div class="row research-project" data-sort="2020-9-6">
		                    <div class="col-md-4">
		                       <a href="publications/Capacitivo.pdf" class="image fit"><img src="images/Capacitivo.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									Capacitivo: Contact-Based Object Recognition on Interactive Fabrics using Capacitive Sensing
		                        </h6>
		                        <p class="text-muted">
		                            <strong>Te-Yen Wu </strong>, Lu Tan, Yuji Zhang, Teddy Seyed, Xing-Dong Yang (UIST 2020)<br>

									<a class="info" href="https://www.youtube.com/watch?v=RjAEHRgEHZI">[Video]</a>
		                            <a class="info" href="https://doi.org/10.1145/3379337.3415829">[DOI]</a>
		                            <a class="info" href="publications/Capacitivo.pdf">[PDF]</a>
		                        </p>
		                       
		                          <p>
									  Capacitivo is a contact-based object recognition technique developed for interactive fabrics, using capacitive sensing. Unlike prior work that has focused on metallic objects, our technique recognizes non-metallic objects such as food, different types of fruits, liquids, and other types of objects that are often found around a home or in a workplace.
		                        </p>
		                      
		                    </div>
		                </div>

		                <div class="row research-project" data-sort="2020-9-6">
		                    <div class="col-md-4">
		                       <a href="publications/Fabriccio.pdf" class="image fit"><img src="images/Fabriccio.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									Fabriccio: Touchless Gestural Input on Interactive Fabrics
		                        </h6>
		                        <p class="text-muted">
		                            <strong>Te-Yen Wu </strong>, Shutong Qi, Junchi Chen, MuJie Shang, Jun Gong, Teddy Seyed, Xing-Dong Yang (CHI 2020)<br>

									<a class="info" href="https://www.youtube.com/watch?v=yX5CPQeXOp4">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3313831.3376681">[DOI]</a>
		                            <a class="info" href="publications/Fabriccio.pdf">[PDF]</a>
		                        </p>
		                          <p>
									  Fabriccio is a  a touchless gesture sensing technique developed for interactive fabrics using Doppler motion sensing.
		                        </p>
		                      
		                    </div>
		                </div>
		                 
		                 <div class="row research-project" data-sort="2020-9-5">
		                    <div class="col-md-4">
		                       <a href="publications/ThreadSense.pdf" class="image fit"><img src="images/ThreadSense.jpg" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									ThreadSense: Locating Touch on an Extremely Thin Interactive Thread
		                        </h6>
		                        <p class="text-muted">
		                            Pin-Sung Ku, Qijia Shao, <strong>Te-Yen Wu </strong>, Jun Gong, Ziyan Zhu, Xia Zhou, Xing-Dong Yang (CHI 2020)<br>

									<a class="info" href="https://www.youtube.com/watch?v=iiSBCE1yCCs">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3313831.3376779">[DOI]</a>
		                            <a class="info" href="publications/ThreadSense.pdf">[PDF]</a>
		                        </p>
		                          <p>
									 We propose a new sensing technique for one-dimensional touch input workable on an interactive thread of less than 0.4 mm thick. Our technique locates up to two touches using impedance sensing with a spacing resolution unachievable by the existing methods.
		                        </p>
		                    </div>
		                </div>
	                <h5>
	                    Wearable Input System
	                </h5>
					<div class="line"></div>
						<div class="row research-project" data-sort="2020-9-5">
		                    <div class="col-md-4">
		                       <a href="publications/Zippro.pdf" class="image fit"><img src="images/Zippro.jpg" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									Zippro: The Design and Implementation of An Interactive Zipper
		                        </h6>
		                         <p class="text-muted">
		                            Pin-Sung Ku, Jun Gong, <strong>Te-Yen Wu </strong>, YiXin Wei, Yiwen Tang, Barrett Ens, Xing-Dong Yang (CHI 2020)<br>

									<a class="info" href="https://www.youtube.com/watch?v=LuLYvN37Fis">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3313831.3376756">[DOI]</a>
		                            <a class="info" href="publications/Zippro.pdf">[PDF]</a>
		                        </p>
		                          <p>
									 This paper explored the possibilities of interaction with ubiquitous zipper-bearing objects, with a focus on opportunities for foreground and background interactions. Based on the findings, we built a self-contained prototype, Zippro that can replace a common zipper slider.
		                        </p>
		                       
		                    </div>
		                </div>
		                <div class="row research-project" data-sort="2020-9-5">
		                    <div class="col-md-4">
		                       <a href="publications/BiTipText.pdf" class="image fit"><img src="images/BiTipText.jpg" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									BiTipText: Bimanual Eyes-Free Text Entry on a Fingertip Keyboard.
		                        </h6>
		                        <p class="text-muted">
		                            Zheer Xu, Weihao Chen, Dongyang Zhao, Jiehui Luo, <strong>Te-Yen Wu </strong>, Jun Gong, Sicheng Yin, Jialun Zhai, Xing-Dong Yang (CHI 2020)<br>

									<a class="info" href="https://www.youtube.com/watch?v=FzUTmGzAHQw">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3313831.3376306">[DOI]</a>
		                            <a class="info" href="publications/BiTipText.pdf">[PDF]</a>
		                        </p>
		                          <p>
									 We present a bimanual text input method on a miniature fingertip keyboard, that invisibly resides on the first segment of a userâ€™s index finger on both hands.
		                        </p>
		                    </div>
		                </div>
		                <div class="row research-project" data-sort="2019-9-5">
		                    <div class="col-md-4">
		                       <a href="publications/TipText.pdf" class="image fit"><img src="images/Tiptext.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									TipText: Eyes-Free Text Entry on a Fingertip Keyboard
		                        </h6>
		                         <p class="text-muted">
		                            Zheer Xu, Pui Chung Wong, Jun Gong, <strong>Te-Yen Wu </strong>, Aditya Shekhar Nittala, Xiaojun Bi, Jurgen Steimle, Hongbo Fu, Kening Zhu, Xing-Dong Yang (UIST 2019)<br>

									<a class="info" href="https://www.youtube.com/watch?v=i3YPZsiHEKM">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3332165.3347865">[DOI]</a>
		                            <a class="info" href="publications/TipText.pdf">[PDF]</a>
		                        </p>
		                         <p class="text-award">
		                            <i class="fas fa-trophy"></i> Best Paper Award
		                        </p>
		                          <p>
									 In this paper, we propose and investigate a new text entry technique using micro thumb-tip gestures. Our technique features a miniature QWERTY keyboard residing invisibly on the first segment of the userâ€™s index finger. Text entry can be carried out using the thumb-tip to tap the tip of the index finger. 
		                        </p>
		                       
		                       
		                      
		                    </div>
		                </div>
					<h5>
	                    Accessibility and Educational Tools
	                </h5>
					<div class="line"></div>
		                <div class="row research-project" data-sort="2021-1-6">
		                    <div class="col-md-4">
		                       <a href="publications/AccessibleCircuit.pdf" class="image fit"><img src="images/AccessibleCircuit.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									AccessibleCircuit: Adaptive Add-On Circuit Components for People with Blindness or Low Vision
		                        </h6>
		                        <p class="text-muted">
		                            Ruei-Che Chang, Wen-Ping Wang, Chi-Huan Chiang, <strong>Te-Yen Wu </strong>, Zheer Xu, Justin Luo, Bing-Yu Chen, Xing-Dong Yang (CHI 2021)<br>

									<a class="info" href="https://www.youtube.com/watch?v=YIYCwiUboqA">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3411764.3445690">[DOI]</a>
		                            <a class="info" href="publications/AccessibleCircuit.pdf">[PDF]</a>
		                        </p>
		                          <p>
									  In this paper, we propose the designs for low cost and 3D-printable add-on components to adapt existing breadboards, circuit components and electronics tools for blind or low vision (BLV) users.
		                        </p>
		                      
		                    </div>
		                </div>
		                 
		                <div class="row research-project" data-sort="2020-9-5">
		                    <div class="col-md-4">
		                       <a href="publications/TangibleCircuits.pdf" class="image fit"><img src="images/TangibleCircuits.jpg" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									TangibleCircuits: An Interactive 3D Printed Circuit Education Tool for People with Visual Impairments.
		                        </h6>
		                        <p class="text-muted">
		                            Josh Urban Davis, <strong>Te-Yen Wu </strong>, Bo Shi, Hanyi Lui, Anthina Panotopoulou, Emily Whiting, Xing-Dong Yang (CHI 2020)<br>

									<a class="info" href="https://www.youtube.com/watch?v=iiSBCE1yCCs">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3313831.3376779">[DOI]</a>
		                            <a class="info" href="publications/ThreadSense.pdf">[PDF]</a>
		                        </p>
		                        <p class="text-award">
		                            <i class="fas fa-medal"></i> Honorable Mention Award
		                        </p>
		                          <p>
									 We present a novel haptic and audio feedback device that allows blind and visually impaired (BVI) users to understand circuit diagrams. TangibleCircuits allows users to interact with a 3D printed tangible model of a circuit which provides audio tutorial directions while being touched. 
		                        </p>
		                    </div>
		                </div>
		                <div class="row research-project" data-sort="2019-1-5">
		                    <div class="col-md-4">
		                       <a href="publications/Proxino.pdf" class="image fit"><img src="images/Proxino.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									Proxino: Enabling Prototyping of Virtual Circuits With Physical Proxies
		                        </h6>
		                        <p class="text-muted">
		                            <strong>Te-Yen Wu </strong>, Jun Gong, Teddy Seyed, Xing-Dong Yang (UIST 2019)<br>

									<a class="info" href="https://www.youtube.com/watch?v=6fe11Kizwko&t=1s">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/10.1145/3332165.3347938">[DOI]</a>
		                            <a class="info" href="publications/Proxino.pdf">[PDF]</a>
		                        </p>
		                          <p>
									 In this paper, we propose blending the virtual and physical worlds for prototyping circuits using physical proxies. With physical proxies, real-world components (e.g. a motor, or light sensor) can be used with a virtual counterpart for a circuit designed in software. 
		                        </p>
		                    </div>
		                </div>
		                <div class="row research-project" data-sort="2016-11-5">
		                    <div class="col-md-4">
		                       <a href="publications/CurrentViz.pdf" class="image fit"><img src="images/CurrentViz.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									CurrentViz: Sensing and Visualizing Electric Current Flows of Breadboarded Circuits.
		                        </h6>
		                        <p class="text-muted">
		                            <strong>Te-Yen Wu </strong>, Hao-Ping Shen, Yu-Chian Wu, Yu-An Chen, Pin-Sung Ku, Ming-Wei Hsu, Jun-You Liu, Yu-Chih Lin, Mike Y. Chen (UIST 2017)<br>

									<a class="info" href="https://videopress.com/v/yzSwNbOl">[Video]</a>
		                            <a class="info" href="https://doi.org/10.1145/3126594.3126646">[DOI]</a>
		                            <a class="info" href="publications/CurrentViz.pdf">[PDF]</a>
		                        </p>
		                          <p>
									We present CurrentViz, a system that can sense and visualize the electric current flowing through a circuit, which helps users quickly understand otherwise invisible circuit behavior. 
		                        </p>
		                    </div>
		                </div>
		                
		                <div class="row research-project" data-sort="2016-11-5">
		                    <div class="col-md-4">
		                       <a href="publications/CircuitSense.pdf" class="image fit"><img src="images/CircuitSense.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									CircuitSense: Automatic Sensing of Physical Circuits and Generation of Virtual Circuits to Support Software Tools.
		                        </h6>
		                        <p class="text-muted">
		                            <strong>Te-Yen Wu </strong>, Bryan Wang, Jiun-Yu Lee, Hao-Ping Shen, Yu-Chian Wu, Yu-An Chen, Pin-Sung Ku, Ming-Wei Hsu, Yu-Chih Lin, Mike Y. Chen (UIST 2017)<br>

									<a class="info" href="https://videopress.com/v/rOTRMKXM">[Video]</a>
		                            <a class="info" href="https://doi.org/10.1145/3126594.3126634">[DOI]</a>
		                            <a class="info" href="publications/CircuitSense.pdf">[PDF]</a>
		                        </p>
		                          <p>
									 CircuitSense is a system that automatically recognizes the wires and electronic components placed on breadboards.
		                        </p>
		                    </div>
		                </div>
		                <div class="row research-project" data-sort="2015-11-5">
		                    <div class="col-md-4">
		                       <a href="publications/CircuitStack.pdf" class="image fit"><img src="images/CircuitStack.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									CircuitStack: Supporting Rapid Prototyping and Evolution of Electronic Circuits
		                        </h6>
		                        <p class="text-muted">
		                            Chiuan Wang, Hsuan-Ming Yeh, Bryan Wang, <strong>Te-Yen Wu </strong>, Hsin-Ruey Tsai, Rong-Hao Liang, Yi-Ping Hung, Mike Y. Chen (UIST 2016)<br>

									<a class="info" href="https://videopress.com/v/2ZdEiZP2">[Video]</a>
		                            <a class="info" href="https://doi.org/10.1145/2984511.2984527">[DOI]</a>
		                            <a class="info" href="publications/CircuitStack.pdf">[PDF]</a>
		                        </p>
		                          <p>
									CircuitStack is a system that com- bines the flexibility of breadboarding with the correctness of printed circuits, for enabling rapid and extensible circuit con- struction. 
		                        </p>
		                    </div>
		                </div>
	                </div>
					<h5>
	                   AR/VR Interactions and Others
	                </h5>
					<div class="line"></div>


                	<div class="row research-project" data-sort="2017-11-5">
	                    <div class="col-md-4">
	                       <a href="publications/SpeechBubbles.pdf" class="image fit"><img src="images/SpeechBubbles.png" alt=""/></a>
	                    </div>
	                    <div class="col-md-8">
	                        <h6>
								SpeechBubbles: Enhancing Captioning Experiences for Deaf and Hard-of-Hearing People in Group Conversations
	                        </h6>
	                        <p class="text-muted">
	                            Yi-Hao Peng, Ming-Wei Hsu, Paul Taele, Ting-Yu Lin, Po-En Lai, Leon Hsu, Tzu-chuan Chen, <strong>Te-Yen Wu </strong>, Yu-An Chen, Hsien-Hui Tang, Mike Y. Chen (CHI 2018)<br>

								<a class="info" href="https://videopress.com/v/LFwbAOYD">[Video]</a>
	                            <a class="info" href="https://doi.org/10.1145/3173574.3173867">[DOI]</a>
	                            <a class="info" href="publications/SpeechBubbles.pdf">[PDF]</a>
	                        </p>
	                          <p>
								 In this paper, we interviewed and co-designed with eight DHH participants to address the following challenges: 1) associating utterances with speakers, 2) ordering utterances from different speakers, 3) displaying optimal content length, and 4) visualizing utterances from out-of-view speakers.
	                        </p>
	                    </div>
	                </div>
	                <div class="row research-project" data-sort="2017-11-5">
	                    <div class="col-md-4">
	                       <a href="publications/ARPilot.pdf" class="image fit"><img src="images/ARPilot.png" alt=""/></a>
	                    </div>
	                    <div class="col-md-8">
	                        <h6>
								ARPilot: Designing and Investigating AR Shooting Interfaces on Mobile Devices
	                        </h6>
	                        <p class="text-muted">
	                            Yu-An Chen, <strong>Te-Yen Wu </strong>, Tim Chang, Jun You Liu, Yuan-Chang Hsieh, Leon Yulun Hsu, Ming-Wei Hsu, Paul Taele, Neng-Hao Yu, Mike Y. Chen (MobileHCI 2018)<br>

								<a class="info" href="https://videopress.com/v/kSdpnxr8">[Video]</a>
	                            <a class="info" href="https://doi.org/10.1145/3229434.3229475">[DOI]</a>
	                            <a class="info" href="publications/ARPilot.pdf">[PDF]</a>
	                        </p>
	                          <p>
								 We present an AR direct-manipulation interface that lets users plan an aerial video by physically moving their mobile devices around a miniature 3D model of the scene, shown via Augmented Reality (AR). 
	                        </p>
	                    </div>
	                </div>
	                <div class="row research-project" data-sort="2022-9-6">
	                    <div class="col-md-4">
	                       <a href="publications/NFCStack.pdf" class="image fit"><img src="images/NFCStack.png" alt=""/></a>
	                    </div>
	                    <div class="col-md-8">
	                        <h6>
								NFCStack: Identifiable Physical Building Blocks that Support Concurrent Construction and Frictionless Interaction
	                        </h6>
	                        <p class="text-muted">
	                            Chi-Jung Lee, Rong-Hao Liang, Ling-Chien Yang, Chi-Huan Chiang, <strong>Te-Yen Wu </strong>, Bing-Yu Chen (UIST 2022)<br>

								<a class="info" href="">[Video]</a>
	                            <a class="info" href="">[DOI]</a>
	                            <a class="info" href="publications/NFCStack.pdf">[PDF]</a>
	                        </p>
	                          <p>
								 NFCStack is a physical building block system that can support stacking and frictionless interaction based on near-field communication (NFC). 
	                        </p>
	                    </div>
	                </div>
	                <div class="row research-project" data-sort="2017-11-5">
	                    <div class="col-md-4">
	                       <a href="publications/ActiveErgo.pdf" class="image fit"><img src="images/ActiveErgo.png" alt=""/></a>
	                    </div>
	                    <div class="col-md-8">
	                        <h6>
								ActiveErgo: Automatic and Personalized Ergonomics using Self-actuating Furniture.
	                        </h6>
	                        <p class="text-muted">
	                            Yu-Chian Wu, <strong>Te-Yen Wu </strong>, Paul Taele, Bryan Wang, Jun-You Liu, Ping-sung Ku, Po-en Lai, Mike Y. Chen (CHI 2018)<br>

								<a class="info" href="https://videopress.com/v/8Rx2wY3X">[Video]</a>
	                            <a class="info" href="https://doi.org/10.1145/3173574.3174132">[DOI]</a>
	                            <a class="info" href="publications/ActiveErgo.pdf">[PDF]</a>
	                        </p>
	                          <p>
								 We present ActiveErgo, the first active approach to improving ergonomics by combining sensing and actuation of motorized furniture. It provides automatic and personalized ergonomics of computer workspaces in accordance to the recommended ergonomics guidelines. 
	                        </p>
	                    </div>
	                </div>

	            </div>
	            <!-- /left column -->
				
	            <!-- right column -->
	            <div class="col-lg-4 mb-2">
	            	<h4>Work Experience</h4>
	                <ul class="news" style="font-size: 13px">
						<li>Research Scientist Intern, Meta Reality Labs, 2022</li>
						<li>Research Scientist Intern, Microsoft Research, 2021</li>
						<li>Research Scientist Intern, Microsoft Research, 2020</li>
						<li>Android App Intern, Yahoo, 2017</li>
						<li>Fullstack Software Engineer, Bearsoft Inc, 2015</li>
						<li>Founder, Hydrabrain Game Studio, 2014</li>
	                </ul>
					<br>

	                <h4>Latest News</h4>
	                <ul class="news" style="font-size: 13px">
	                	<li>Oct 31. Talk and Demo at UIST 2022. </li>
	                	<li>Oct 4.Invited talk at Autodesk. </li>
	                	<li>Sept 15. One submission to CHI. </li>
						<li>Sept 5. Preparing job documents.</li>
						<li>Sept 1. Ending internship at Meta.</li>
						<li>July 29. iWood is offially accepted by UIST 2022.</li>
						<li>June 13. Starting internship at Meta.</li>
	                </ul>
					<br>

	                <!-- <h2>Teaching</h2>
	                <ul class="news" style="font-size: 13px">
						<li><a href=""></a>.</li>
	                </ul> -->
					<br>
					
<!-- 	                <h2>Awards</h2>
	                <ul class="news" style="font-size: 13px">
	                </ul>
					<br> -->
					
	                <h4>Service</h4>
					
					<p style="font-size: 13px">
						Conference Review: CHI'19 - '22, UIST'19 - '22, ISS'20, CSCW'21, TEI'20 - '21, MobileHCI'22 <br>
						Journal Review: IMWUT'22 
					</p>
					
					<br>
	                
	 				<a class="twitter-timeline" data-width="400" data-height="600" data-theme="light" href="https://twitter.com/TeYenWu2?ref_src=twsrc%5Etfw">Tweets by TeYenWu</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
	                <br>
<!-- 					<h4>Press</h4>
	                <ul class="news" style="font-size: 13px">
						
						<li>NSF News: <a href="https://beta.nsf.gov/news/fitbit-face-can-turn-any-face-mask-smart-monitoring-device">'Fitbit for the face' can turn any face mask into smart monitoring device</a></li>
						
	                </ul> -->
	            </div>
            <!-- /right column -->
        </div>
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/jquery.scrolly.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>

	</body>
</html>
