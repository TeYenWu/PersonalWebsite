<!DOCTYPE HTML>
<html>
	<head>
		<title>Te-Yen Wu</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<meta name="description" content="Te-Yen Wu is a PhD candidate at Dartmouth College">
		<!-- <base href="https://teyenwu.com/"> -->
		
		<!-- <link rel="stylesheet" href="assets/css/main.css" /> -->
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.7/css/all.min.css">
		<link rel="stylesheet" href="assets/css/custom-style.css" />

		<meta charset="utf-8">
		<!-- Google tag (gtag.js) -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-1LXFW6JX1J"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'G-1LXFW6JX1J');
		</script>

		<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.0/umd/popper.min.js"></script>
		<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.1.0/js/bootstrap.min.js"></script>

		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>

	</head>
	<body class="is-preload">
		 <nav class="navbar fixed-top navbar-expand-lg navbar-light" style="background-color: #722d3d;">
		  <a class="navbar-brand" href="https://www.cs.fsu.edu" style = "padding:0; font-size:0"><img alt="" width="30" class="" src="images/FSU.png"/></a>
		  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
		    <span class="navbar-toggler-icon"></span>
		  </button>
		  <div class="collapse navbar-collapse" id="navbarNav">
		    <ul class="navbar-nav ml-auto">
		      <li class="nav-item active">
		        <a class="nav-link" style = "color: #FFFFFF" href="#">Home <span class="sr-only">(current)</span></a>
		      </li>
		      <li class="nav-item">
		        <a class="nav-link" style = "color: #FFFFFF" href="publications/TeYenWu.pdf">CV/Resume</a>
		      </li>
		      <li class="nav-item">
		        <a class="nav-link" style = "color: #FFFFFF" href="courses/index.html">Courses</a>
		      </li>
		      <li class="nav-item">
		        <a class="nav-link" style = "color: #FFFFFF" href="https://scholar.google.com/citations?hl=en&user=TMrlaiUAAAAJ">Google Scholar</a>
		      </li>
		      <li class="nav-item">
		        <a class="nav-link" style = "color: #FFFFFF" href="#publications">Publications</a>
		      </li>
		      <li class="nav-item">
		        <a class="nav-link" style = "color: #FFFFFF" href="mailto:twu@cs.fsu.edu">Email</a>
		      </li>
		    </ul>
		  </div>
		</nav>
		<!-- <div class="container" style="margin-top: 100px">
            <div class="row flex-row-reverse">
            	<div class="col-2 col-12-medium">
                    <div class="about-avatar">
                        <span class="image fit"><img src="images/TeYenWu(prof).jpg" alt="" /></span>
                    </div>
                </div>
                <div class="col-10 col-12-medium">
                    <div class="about-text go-to text-justify">
                        <h3 class="dark-color">Te-Yen Wu</h3>
                        <h6 class="theme-color lead">Assistant Professor at Florida State University</h6>
	                    <p> My research lies in the intersection areas of Human-Computer Interaction, Cyber-Physical Systems, and AI Technologies. 
							My career goal is to build <strong>sustainable</strong>, <strong>scalable</strong> and <strong>intelligent</strong> ambient computing to enable the creation of smart environments on an unprecedented scale.
							To achieve the goal, I have developed smart everyday materials that can 1) seamlessly sense user activities and contexts, 2) be used with established methods to create a smart environment, and 3) operate without embedded batteries and silicon-based integrated circuits.
							In addition, I have also worked on other HCI topics, such as wearable sensing technology, text entry system and Human-AI interactions in VR/AR.
							My research is generally published in top HCI venues like CHI and UIST, while I also contribute to top AI conferences such as ICLR. 
							My research has attracted considerable public interests via Internet News (e.g. Engadget, Times). 
							Currently, I direct the <a href="https://www.cs.fsu.edu/~twu/makex/index.html"><strong>MakeX Lab</strong></a> at FSU.
							I am looking for PhD students </strong> who are highly motivated and interested in Human-Computer Interaction research.
							A background in hardware is advantageous but not mandatory. 
							If you're interested in working with me, please send an email with your CV and a one-page research statement outlining the topic you wish to explore.
						</p>
                    </div>
                </div>
                
            </div>
        </div> -->
		<div class="container" style="margin-top: 50px; margin-bottom: 50px">
			 <h2 >Research Overview</h2> 
            <div class="row taxonomy" style="margin-top:20px;">
	            <div class="col-lg-6 mb-6">
	                <img src="images/FutureMaterials.png">
	            </div>
				
				<div style="margin-top:0px;" class="col-lg-6 mb-8 text-justify"><p>Ubiquitous computing envisions a future where intelligent devices seamlessly pervade our environments, enhancing convenience and efficiency of our lives.  However, realizing this vision faces three fundamental challenges: <b style="color: blue"> intelligence  (enabling advanced sensing and context-awareness in devices)</b>, <b style="color: #FF5733">scalability (allowing mass production and seamless deployment without compromising performance) </b>, and <b style="color: green">sustainability  (ensuring energy efficiency and minimal environmental impact)</b>. To address these challenges, my research develops a new generation of ambient computing known as smart materials. These materials possess computational functionalities such as sensing, computing and wireless communication, while retaining the look and feel of conventional materials, thus can be seamlessly integrated into environments. They are designed to withstand various fabrication processes such as cutting and screwing. This allows the existing workforce to effortlessly incorporate them into established construction processes just like regular materials, facilitating the deployment on an unprecedented scale. Moreover, the unobtrusive integration of smart materials into environments allows for broader coverage, empowering them to gather enough energy from their expansive surroundings for self-sustained operation. This enables these materials to be used over long-time without compromising significant energy in the environment.</p> </div>

				
        	</div> 
       </div>
       <div class="container" style="margin-top: 50px; margin-bottom: 50px">
	       <div class="row">
	            <!-- left column -->
	            <div class="col-lg-8 mb-2">
	                <h2>
	                    Selected Publications
	                </h2>
	                <br/>

	                <div class="publications" id="publications">
		                <h5>
	                    Smart Materials and Objects
		                </h5>
						<div class="line"></div>
						<div class="row research-project" data-sort="2023-9-7">
		                    <div class="col-md-4">
		                       <img src="images/Tagnoo.png" alt=""/>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									Tagnoo: Enabling Smart Room-Scale Environments with RFID-Augmented Plywood
		                        </h6>
		                         <p class="text-muted">
									Yuning Su, Tingyu Zhang, Jiuen Feng, Yonghao Shi, Xing-Dong Yang, <strong>Te-Yen Wu </strong> (CHI 2024) <br>
		                            
									<a class="info" href="publications/Tagnoo.pdf">[PDF]</a>
									<!-- <a class="info" href="https://www.youtube.com/watch?v=MELff8HCPHg">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3526113.3545640">[DOI]</a>
		                            <a class="info" href="publications/iWood.pdf">[PDF]</a>
		                            <a class="info" href="https://github.com/TeYenWu/iWood">[Github]</a> -->
		                        </p>
		                          <p>
									Tagnoo is a computational plywood augmented with RFID tags, aimed at empowering woodworkers to effortlessly create room-scale smart environments. Unlike existing solutions, Tagnoo does not necessitate technical expertise or disrupt established woodworking routines. This battery-free and cost-effective solution seamlessly integrates computation capabilities into plywood, while preserving its original appearance and functionality.

		                        	</p>
		                      
		                    </div>
		                </div>
						<div class="row research-project" data-sort="2023-9-7">
		                    <div class="col-md-4">
		                       <img src="images/WooDowel.png" alt=""/>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									WooDowel: Enhancing Triboelectric Plywood Sensors with Electromagnetic Shielding
		                        </h6>
		                         <p class="text-muted">
									Yonghao Shi, Chenzheng Li, Yuning Su, Xing-Dong Yang, <strong>Te-Yen Wu </strong> (CHI 2024) <br>
		                            
									<a class="info" href="publications/WooDowel.pdf">[PDF]</a>
									<!-- <a class="info" href="https://www.youtube.com/watch?v=MELff8HCPHg">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3526113.3545640">[DOI]</a>
		                            <a class="info" href="publications/iWood.pdf">[PDF]</a>
		                            <a class="info" href="https://github.com/TeYenWu/iWood">[Github]</a> -->
		                        </p>
		                          <p>
									WooDowel presents a new approach that enables the woodworker to manually isolate short-circuited electrodes. This method facilitates the creation of sensors using overlapping electrodes, while also incorporating EM shielding, thereby resulting in a substantial improvement in the sensor's robustness when detecting user activities. 

		                        </p>
		                      
		                    </div>
		                </div>
		                <div class="row research-project" data-sort="2022-9-7">
		                    <div class="col-md-4">
		                       <a href="publications/iWood.pdf" class="image fit"><img src="images/iWood.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									iWood: Makeable Vibration Sensor for Interactive Plywood
		                        </h6>
		                         <p class="text-muted">
		                            <strong>Te-Yen Wu </strong>, Xing-Dong Yang. (UIST 2022)<br>

									<a class="info" href="https://www.youtube.com/watch?v=MELff8HCPHg">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3526113.3545640">[DOI]</a>
		                            <a class="info" href="publications/iWood.pdf">[PDF]</a>
		                            <a class="info" href="https://github.com/TeYenWu/iWood">[Github]</a>
		                        </p>
		                          <p>
									iWood is interactive plywood that can sense vibration based on triboelectric effect. As a material, iWood survives common woodworking operations, such as sawing, screwing, and nailing and can be used to create furniture and artifacts.
		                        </p>
		                      
		                    </div>
		                </div>
		               
						<div class="row research-project" data-sort="2022-5-6">
		                    <div class="col-md-4">
		                       <a href="publications/NFCe.pdf" class="image fit"><img src="images/NFCe.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									Body-Centric NFC: Body-Centric Interaction with NFC Devices Through Near-Field Enabled Clothing
		                        </h6>
		                        <p class="text-muted">
		                            <strong>Te-Yen Wu </strong>, Huizhong Ye, Chi-Jung Lee, Xing-Dong Yang, Bing-Yu Chen, Rong-Hao Liang(DIS 2022)<br>

									<a class="info" href="https://vimeo.com/725012677">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/10.1145/3532106.3534569">[DOI]</a>
		                            <a class="info" href="publications/NFCe.pdf">[PDF]</a>
		                        </p>
		                          <p>
									  This paper presents an investigation of body-centric interactions between the NFC device users and their surroundings, and an accessible method for fabricating fexible, extensible, and scalable NFC extenders on clothing pieces, and an easy-to-use toolkit for facilitating designers to realize the interactive experiences.
		                        </p>
		                    </div>
		                </div>
		                <div class="row research-project" data-sort="2021-1-6">
		                    <div class="col-md-4">
		                       <a href="https://www.microsoft.com/en-us/research/publication/project-tasca-enabling-touch-and-contextual-interactions-with-a-pocket-based-textile-sensor/" class="image fit"><img src="images/Tasca.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									Project Tasca: Enabling Touch and Contextual Interactions with a Pocket-based Textile Sensor
		                        </h6>
		                        <p class="text-muted">
		                            <strong>Te-Yen Wu </strong>, Zheer Xu, Xing-Dong Yang, Steve Hodges, Teddy Seyed(CHI 2021)<br>

									<a class="info" href="https://www.youtube.com/watch?v=SsLtLPfw_LE">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/10.1145/3411764.3445712">[DOI]</a>
		                            <a class="info" href="https://www.microsoft.com/en-us/research/publication/project-tasca-enabling-touch-and-contextual-interactions-with-a-pocket-based-textile-sensor/">[PDF]</a>
		                        </p>
		                       
		                          <p>
									   Project Tasca presents a pocket-based textile sensor that detects user input and recognizes everyday objects usually carried in the pockets of a pair of pants (e.g., keys, coins, electronic devices, or plastic items). By creating a new fabric-based sensor capable of detecting in-pocket touch and pressure, and recognizing metallic, non-metallic, and tagged objects inside the pocket, we enable a rich variety of subtle, eyes-free, and always-available input, as well as context-driven interactions in wearable scenarios
		                        </p>

		                    </div>
		                </div>
		                <div class="row research-project" data-sort="2020-9-6">
		                    <div class="col-md-4">
		                       <a href="publications/Capacitivo.pdf" class="image fit"><img src="images/Capacitivo.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									Capacitivo: Contact-Based Object Recognition on Interactive Fabrics using Capacitive Sensing
		                        </h6>
		                        <p class="text-muted">
		                            <strong>Te-Yen Wu </strong>, Lu Tan, Yuji Zhang, Teddy Seyed, Xing-Dong Yang (UIST 2020)<br>

									<a class="info" href="https://www.youtube.com/watch?v=RjAEHRgEHZI">[Video]</a>
		                            <a class="info" href="https://doi.org/10.1145/3379337.3415829">[DOI]</a>
		                            <a class="info" href="publications/Capacitivo.pdf">[PDF]</a>
		                        </p>
		                       
		                          <p>
									  Capacitivo is a contact-based object recognition technique developed for interactive fabrics, using capacitive sensing. Unlike prior work that has focused on metallic objects, our technique recognizes non-metallic objects such as food, different types of fruits, liquids, and other types of objects that are often found around a home or in a workplace.
		                        </p>
		                      
		                    </div>
		                </div>

		                <div class="row research-project" data-sort="2020-9-6">
		                    <div class="col-md-4">
		                       <a href="publications/Fabriccio.pdf" class="image fit"><img src="images/Fabriccio.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									Fabriccio: Touchless Gestural Input on Interactive Fabrics
		                        </h6>
		                        <p class="text-muted">
		                            <strong>Te-Yen Wu </strong>, Shutong Qi, Junchi Chen, MuJie Shang, Jun Gong, Teddy Seyed, Xing-Dong Yang (CHI 2020)<br>

									<a class="info" href="https://www.youtube.com/watch?v=yX5CPQeXOp4">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3313831.3376681">[DOI]</a>
		                            <a class="info" href="publications/Fabriccio.pdf">[PDF]</a>
		                        </p>
		                          <p>
									  Fabriccio is a  a touchless gesture sensing technique developed for interactive fabrics using Doppler motion sensing.
		                        </p>
		                      
		                    </div>
		                </div>
		                 
		                 <div class="row research-project" data-sort="2020-9-5">
		                    <div class="col-md-4">
		                       <a href="publications/ThreadSense.pdf" class="image fit"><img src="images/ThreadSense.jpg" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									ThreadSense: Locating Touch on an Extremely Thin Interactive Thread
		                        </h6>
		                        <p class="text-muted">
		                            Pin-Sung Ku, Qijia Shao, <strong>Te-Yen Wu </strong>, Jun Gong, Ziyan Zhu, Xia Zhou, Xing-Dong Yang (CHI 2020)<br>

									<a class="info" href="https://www.youtube.com/watch?v=iiSBCE1yCCs">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3313831.3376779">[DOI]</a>
		                            <a class="info" href="publications/ThreadSense.pdf">[PDF]</a>
		                        </p>
		                          <p>
									 We propose a new sensing technique for one-dimensional touch input workable on an interactive thread of less than 0.4 mm thick. Our technique locates up to two touches using impedance sensing with a spacing resolution unachievable by the existing methods.
		                        </p>
		                    </div>
		                </div>
						<div class="row research-project" data-sort="2020-9-5">
		                    <div class="col-md-4">
		                       <a href="publications/Zippro.pdf" class="image fit"><img src="images/Zippro.jpg" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									Zippro: The Design and Implementation of An Interactive Zipper
		                        </h6>
		                         <p class="text-muted">
		                            Pin-Sung Ku, Jun Gong, <strong>Te-Yen Wu </strong>, YiXin Wei, Yiwen Tang, Barrett Ens, Xing-Dong Yang (CHI 2020)<br>

									<a class="info" href="https://www.youtube.com/watch?v=LuLYvN37Fis">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3313831.3376756">[DOI]</a>
		                            <a class="info" href="publications/Zippro.pdf">[PDF]</a>
		                        </p>
		                          <p>
									 This paper explored the possibilities of interaction with ubiquitous zipper-bearing objects, with a focus on opportunities for foreground and background interactions. Based on the findings, we built a self-contained prototype, Zippro that can replace a common zipper slider.
		                        </p>
		                       
		                    </div>
		                </div>
                	<h5>
	                   Text Entry System
	                </h5>
					<div class="line"></div>
		                <div class="row research-project" data-sort="2020-9-5">
		                    <div class="col-md-4">
		                       <a href="publications/BiTipText.pdf" class="image fit"><img src="images/BiTipText.jpg" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									BiTipText: Bimanual Eyes-Free Text Entry on a Fingertip Keyboard.
		                        </h6>
		                        <p class="text-muted">
		                            Zheer Xu, Weihao Chen, Dongyang Zhao, Jiehui Luo, <strong>Te-Yen Wu </strong>, Jun Gong, Sicheng Yin, Jialun Zhai, Xing-Dong Yang (CHI 2020)<br>

									<a class="info" href="https://www.youtube.com/watch?v=FzUTmGzAHQw">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3313831.3376306">[DOI]</a>
		                            <a class="info" href="publications/BiTipText.pdf">[PDF]</a>
		                        </p>
		                          <p>
									 We present a bimanual text input method on a miniature fingertip keyboard, that invisibly resides on the first segment of a user’s index finger on both hands.
		                        </p>
		                    </div>
		                </div>
		                <div class="row research-project" data-sort="2019-9-5">
		                    <div class="col-md-4">
		                       <a href="publications/TipText.pdf" class="image fit"><img src="images/Tiptext.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									TipText: Eyes-Free Text Entry on a Fingertip Keyboard
		                        </h6>
		                         <p class="text-muted">
		                            Zheer Xu, Pui Chung Wong, Jun Gong, <strong>Te-Yen Wu </strong>, Aditya Shekhar Nittala, Xiaojun Bi, Jurgen Steimle, Hongbo Fu, Kening Zhu, Xing-Dong Yang (UIST 2019)<br>

									<a class="info" href="https://www.youtube.com/watch?v=i3YPZsiHEKM">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3332165.3347865">[DOI]</a>
		                            <a class="info" href="publications/TipText.pdf">[PDF]</a>
		                        </p>
		                         <p class="text-award">
		                            <i class="fas fa-trophy"></i> Best Paper Award
		                        </p>
		                          <p>
									 In this paper, we propose and investigate a new text entry technique using micro thumb-tip gestures. Our technique features a miniature QWERTY keyboard residing invisibly on the first segment of the user’s index finger. Text entry can be carried out using the thumb-tip to tap the tip of the index finger. 
		                        </p>
		                       
		                       
		                      
		                    </div>
		                </div>
					<h5>
	                    Prototyping Tools
	                </h5>
					<div class="line"></div>
		                <div class="row research-project" data-sort="2021-1-6">
		                    <div class="col-md-4">
		                       <a href="publications/AccessibleCircuit.pdf" class="image fit"><img src="images/AccessibleCircuit.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									AccessibleCircuit: Adaptive Add-On Circuit Components for People with Blindness or Low Vision
		                        </h6>
		                        <p class="text-muted">
		                            Ruei-Che Chang, Wen-Ping Wang, Chi-Huan Chiang, <strong>Te-Yen Wu </strong>, Zheer Xu, Justin Luo, Bing-Yu Chen, Xing-Dong Yang (CHI 2021)<br>

									<a class="info" href="https://www.youtube.com/watch?v=YIYCwiUboqA">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3411764.3445690">[DOI]</a>
		                            <a class="info" href="publications/AccessibleCircuit.pdf">[PDF]</a>
		                        </p>
		                          <p>
									  In this paper, we propose the designs for low cost and 3D-printable add-on components to adapt existing breadboards, circuit components and electronics tools for blind or low vision (BLV) users.
		                        </p>
		                      
		                    </div>
		                </div>
		                 
		                <div class="row research-project" data-sort="2020-9-5">
		                    <div class="col-md-4">
		                       <a href="publications/TangibleCircuits.pdf" class="image fit"><img src="images/TangibleCircuits.jpg" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									TangibleCircuits: An Interactive 3D Printed Circuit Education Tool for People with Visual Impairments.
		                        </h6>
		                        <p class="text-muted">
		                            Josh Urban Davis, <strong>Te-Yen Wu </strong>, Bo Shi, Hanyi Lui, Anthina Panotopoulou, Emily Whiting, Xing-Dong Yang (CHI 2020)<br>

									<a class="info" href="https://www.youtube.com/watch?v=iiSBCE1yCCs">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3313831.3376779">[DOI]</a>
		                            <a class="info" href="publications/ThreadSense.pdf">[PDF]</a>
		                        </p>
		                        <p class="text-award">
		                            <i class="fas fa-medal"></i> Honorable Mention Award
		                        </p>
		                          <p>
									 We present a novel haptic and audio feedback device that allows blind and visually impaired (BVI) users to understand circuit diagrams. TangibleCircuits allows users to interact with a 3D printed tangible model of a circuit which provides audio tutorial directions while being touched. 
		                        </p>
		                    </div>
		                </div>
		                <div class="row research-project" data-sort="2019-1-5">
		                    <div class="col-md-4">
		                       <a href="publications/Proxino.pdf" class="image fit"><img src="images/Proxino.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									Proxino: Enabling Prototyping of Virtual Circuits With Physical Proxies
		                        </h6>
		                        <p class="text-muted">
		                            <strong>Te-Yen Wu </strong>, Jun Gong, Teddy Seyed, Xing-Dong Yang (UIST 2019)<br>

									<a class="info" href="https://www.youtube.com/watch?v=6fe11Kizwko&t=1s">[Video]</a>
		                            <a class="info" href="https://dl.acm.org/doi/10.1145/3332165.3347938">[DOI]</a>
		                            <a class="info" href="publications/Proxino.pdf">[PDF]</a>
		                        </p>
		                          <p>
									 In this paper, we propose blending the virtual and physical worlds for prototyping circuits using physical proxies. With physical proxies, real-world components (e.g. a motor, or light sensor) can be used with a virtual counterpart for a circuit designed in software. 
		                        </p>
		                    </div>
		                </div>
		                <div class="row research-project" data-sort="2016-11-5">
		                    <div class="col-md-4">
		                       <a href="publications/CurrentViz.pdf" class="image fit"><img src="images/CurrentViz.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									CurrentViz: Sensing and Visualizing Electric Current Flows of Breadboarded Circuits.
		                        </h6>
		                        <p class="text-muted">
		                            <strong>Te-Yen Wu </strong>, Hao-Ping Shen, Yu-Chian Wu, Yu-An Chen, Pin-Sung Ku, Ming-Wei Hsu, Jun-You Liu, Yu-Chih Lin, Mike Y. Chen (UIST 2017)<br>

									<a class="info" href="https://videopress.com/v/yzSwNbOl">[Video]</a>
		                            <a class="info" href="https://doi.org/10.1145/3126594.3126646">[DOI]</a>
		                            <a class="info" href="publications/CurrentViz.pdf">[PDF]</a>
		                        </p>
		                          <p>
									We present CurrentViz, a system that can sense and visualize the electric current flowing through a circuit, which helps users quickly understand otherwise invisible circuit behavior. 
		                        </p>
		                    </div>
		                </div>
		                
		                <div class="row research-project" data-sort="2016-11-5">
		                    <div class="col-md-4">
		                       <a href="publications/CircuitSense.pdf" class="image fit"><img src="images/CircuitSense.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									CircuitSense: Automatic Sensing of Physical Circuits and Generation of Virtual Circuits to Support Software Tools.
		                        </h6>
		                        <p class="text-muted">
		                            <strong>Te-Yen Wu </strong>, Bryan Wang, Jiun-Yu Lee, Hao-Ping Shen, Yu-Chian Wu, Yu-An Chen, Pin-Sung Ku, Ming-Wei Hsu, Yu-Chih Lin, Mike Y. Chen (UIST 2017)<br>

									<a class="info" href="https://videopress.com/v/rOTRMKXM">[Video]</a>
		                            <a class="info" href="https://doi.org/10.1145/3126594.3126634">[DOI]</a>
		                            <a class="info" href="publications/CircuitSense.pdf">[PDF]</a>
		                        </p>
		                          <p>
									 CircuitSense is a system that automatically recognizes the wires and electronic components placed on breadboards.
		                        </p>
		                    </div>
		                </div>
		                <div class="row research-project" data-sort="2015-11-5">
		                    <div class="col-md-4">
		                       <a href="publications/CircuitStack.pdf" class="image fit"><img src="images/CircuitStack.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									CircuitStack: Supporting Rapid Prototyping and Evolution of Electronic Circuits
		                        </h6>
		                        <p class="text-muted">
		                            Chiuan Wang, Hsuan-Ming Yeh, Bryan Wang, <strong>Te-Yen Wu </strong>, Hsin-Ruey Tsai, Rong-Hao Liang, Yi-Ping Hung, Mike Y. Chen (UIST 2016)<br>

									<a class="info" href="https://videopress.com/v/2ZdEiZP2">[Video]</a>
		                            <a class="info" href="https://doi.org/10.1145/2984511.2984527">[DOI]</a>
		                            <a class="info" href="publications/CircuitStack.pdf">[PDF]</a>
		                        </p>
		                          <p>
									CircuitStack is a system that com- bines the flexibility of breadboarding with the correctness of printed circuits, for enabling rapid and extensible circuit con- struction. 
		                        </p>
		                    </div>
		                </div>
	                </div>
					<h5>
	                   AI, AR/VR Interactions and Others
	                </h5>
					<div class="line"></div>

					<div class="row research-project" data-sort="2023-5-23">
		                    <div class="col-md-4">
		                       <a href="https://arxiv.org/abs/2210.05359" class="image fit"><img src="images/Mindeye.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									Mind’s Eye: Grounded Language Model Reasoning through Simulation
		                        </h6>
		                         <p class="text-muted">
		                           Ruibo Liu, Jason Wei, Shixiang Shane Gu, <strong>Te-Yen Wu </strong>, Soroush Vosoughi, Claire Cui, Denny Zhou, Andrew M. Dai (ICLR 2023)<br>

									<a class="info" href="">[Video]</a>
		                            <a class="info" href="">[DOI]</a>
		                            <a class="info" href="https://arxiv.org/abs/2210.05359">[PDF]</a>
		                        </p>
		                          <p>
									 We present Mind’s Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind’s MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning.
		                        </p>
		                      
		                    </div>
		                </div>

					<div class="row research-project" data-sort="2023-4-23">
		                    <div class="col-md-4">
		                       <a href="https://research.facebook.com/publications/xair-a-framework-of-explainable-ai-in-augmented-reality/" class="image fit"><img src="images/XAIR.png" alt=""/></a>
		                    </div>
		                    <div class="col-md-8">
		                        <h6>
									XAIR: A Framework of Explainable AI in Augmented Reality
		                        </h6>
		                         <p class="text-muted">
		                            Xuhai Xu, Mengjie Yu, Tanya Jonker, Kashyap Todi, Feiyu Lu, Xun Qian, João Belo, Tianyi Wang, Michelle Li, Aran Mun, <strong>Te-Yen Wu </strong>, Junxiao Shen, Ting Zhang, Narine Kokhlikyan, Fulton Wang, Paul Sorenson, Sophie Kahyun Kim, Hrvoje Benko (CHI 2023)<br>

									<a class="info" href="">[Video]</a>
		                            <a class="info" href="">[DOI]</a>
		                            <a class="info" href="https://research.facebook.com/publications/xair-a-framework-of-explainable-ai-in-augmented-reality/">[PDF]</a>
		                        </p>
		                          <p>
									 We propose XAIR, a design framework that addresses when, what, and how to provide explanations of AI output in AR. The framework was based on a multi-disciplinary literature review of XAI and HCI research, a large-scale survey probing 500+ end-users’ preferences for AR-based explanations, and three workshops with 12 experts collecting their insights about XAI design in AR. 
		                        </p>
		                      
		                    </div>
		                </div>
                	<div class="row research-project" data-sort="2017-11-5">
	                    <div class="col-md-4">
	                       <a href="publications/SpeechBubbles.pdf" class="image fit"><img src="images/SpeechBubbles.png" alt=""/></a>
	                    </div>
	                    <div class="col-md-8">
	                        <h6>
								SpeechBubbles: Enhancing Captioning Experiences for Deaf and Hard-of-Hearing People in Group Conversations
	                        </h6>
	                        <p class="text-muted">
	                            Yi-Hao Peng, Ming-Wei Hsu, Paul Taele, Ting-Yu Lin, Po-En Lai, Leon Hsu, Tzu-chuan Chen, <strong>Te-Yen Wu </strong>, Yu-An Chen, Hsien-Hui Tang, Mike Y. Chen (CHI 2018)<br>

								<a class="info" href="https://videopress.com/v/LFwbAOYD">[Video]</a>
	                            <a class="info" href="https://doi.org/10.1145/3173574.3173867">[DOI]</a>
	                            <a class="info" href="publications/SpeechBubbles.pdf">[PDF]</a>
	                        </p>
	                          <p>
								 In this paper, we interviewed and co-designed with eight DHH participants to address the following challenges: 1) associating utterances with speakers, 2) ordering utterances from different speakers, 3) displaying optimal content length, and 4) visualizing utterances from out-of-view speakers.
	                        </p>
	                    </div>
	                </div>
	                <div class="row research-project" data-sort="2017-11-5">
	                    <div class="col-md-4">
	                       <a href="publications/ARPilot.pdf" class="image fit"><img src="images/ARPilot.png" alt=""/></a>
	                    </div>
	                    <div class="col-md-8">
	                        <h6>
								ARPilot: Designing and Investigating AR Shooting Interfaces on Mobile Devices
	                        </h6>
	                        <p class="text-muted">
	                            Yu-An Chen, <strong>Te-Yen Wu </strong>, Tim Chang, Jun You Liu, Yuan-Chang Hsieh, Leon Yulun Hsu, Ming-Wei Hsu, Paul Taele, Neng-Hao Yu, Mike Y. Chen (MobileHCI 2018)<br>

								<a class="info" href="https://videopress.com/v/kSdpnxr8">[Video]</a>
	                            <a class="info" href="https://doi.org/10.1145/3229434.3229475">[DOI]</a>
	                            <a class="info" href="publications/ARPilot.pdf">[PDF]</a>
	                        </p>
	                          <p>
								 We present an AR direct-manipulation interface that lets users plan an aerial video by physically moving their mobile devices around a miniature 3D model of the scene, shown via Augmented Reality (AR). 
	                        </p>
	                    </div>
	                </div>
	                <div class="row research-project" data-sort="2022-9-6">
	                    <div class="col-md-4">
	                       <a href="publications/NFCStack.pdf" class="image fit"><img src="images/NFCStack.png" alt=""/></a>
	                    </div>
	                    <div class="col-md-8">
	                        <h6>
								NFCStack: Identifiable Physical Building Blocks that Support Concurrent Construction and Frictionless Interaction
	                        </h6>
	                        <p class="text-muted">
	                            Chi-Jung Lee, Rong-Hao Liang, Ling-Chien Yang, Chi-Huan Chiang, <strong>Te-Yen Wu </strong>, Bing-Yu Chen (UIST 2022)<br>

								<a class="info" href="">[Video]</a>
	                            <a class="info" href="">[DOI]</a>
	                            <a class="info" href="publications/NFCStack.pdf">[PDF]</a>
	                        </p>
	                          <p>
								 NFCStack is a physical building block system that can support stacking and frictionless interaction based on near-field communication (NFC). 
	                        </p>
	                    </div>
	                </div>
	                <div class="row research-project" data-sort="2017-11-5">
	                    <div class="col-md-4">
	                       <a href="publications/ActiveErgo.pdf" class="image fit"><img src="images/ActiveErgo.png" alt=""/></a>
	                    </div>
	                    <div class="col-md-8">
	                        <h6>
								ActiveErgo: Automatic and Personalized Ergonomics using Self-actuating Furniture.
	                        </h6>
	                        <p class="text-muted">
	                            Yu-Chian Wu, <strong>Te-Yen Wu </strong>, Paul Taele, Bryan Wang, Jun-You Liu, Ping-sung Ku, Po-en Lai, Mike Y. Chen (CHI 2018)<br>

								<a class="info" href="https://videopress.com/v/8Rx2wY3X">[Video]</a>
	                            <a class="info" href="https://doi.org/10.1145/3173574.3174132">[DOI]</a>
	                            <a class="info" href="publications/ActiveErgo.pdf">[PDF]</a>
	                        </p>
	                          <p>
								 We present ActiveErgo, the first active approach to improving ergonomics by combining sensing and actuation of motorized furniture. It provides automatic and personalized ergonomics of computer workspaces in accordance to the recommended ergonomics guidelines. 
	                        </p>
	                    </div>
	                </div>

	            </div>
	            <!-- /left column -->
				
	            <!-- right column -->
	            <div class="col-lg-4 mb-2">
	            	<h4>Work Experience</h4>
	                <ul class="news" style="font-size: 13px">
	                	<li>Assistant Professor, Florida State University, 2023</li>
						<li>Research Scientist Intern, Meta Reality Labs, 2022</li>
						<li>Research Scientist Intern, Microsoft Research, 2021</li>
						<li>Research Scientist Intern, Microsoft Research, 2020</li>
						<li>Android App Intern, Yahoo, 2017</li>
						<li>Fullstack Software Engineer, Bearsoft Inc, 2015</li>
						<li>Founder, Hydrabrain Game Studio, 2014</li>
	                </ul>
					<br>

	                <h4>Latest News</h4>
	                <ul class="news" style="font-size: 13px">
						<li>2024 May. Co-advised students, Yuning and Yonghao, will present their works in CHI'24. See you in Hawaii </li>
						<li>2024 Aprl. 3. Two paper submissions to UIST'24. </li>
						<li>2024 Jan. 19. Two paper submissions accepted to CHI'24. </li>
						<li>2023 Sept. 14. Four paper submissions to CHI'24 </li>
	                	<li>2023 Aug. 7. Started AP position at FSU </li>
	                	<li>2023 Aprl 23. Accepted an AP offer from FSU </li>
	                	<li>2022 Dec 9. Invited talk at National Taiwan University. </li>
	                	<li>2022 Nov 9. Invited talk at Salisbury University. </li>
	                	<li>2022 Oct 31. Talk and Demo at UIST 2022. </li>
	                	<li>2022 Oct 4.Invited talk at Autodesk. </li>
	                	<li>2022 Sept 15. One submission to CHI. </li>
						<li>2022 Sept 5. Preparing job documents.</li>
						<li>2022 Sept 1. Ending internship at Meta.</li>
	                </ul>
					<br>

	                <!-- <h2>Teaching</h2>
	                <ul class="news" style="font-size: 13px">
						<li><a href=""></a>.</li>
	                </ul> -->
					<br>
					
<!-- 	                <h2>Awards</h2>
	                <ul class="news" style="font-size: 13px">
	                </ul>
					<br> -->
					
	                <h4>Service</h4>
					
					<p style="font-size: 13px">
						Conference Organinzing Committee: UIST'23, UIST'24<br>
						Program/Associate Chairs: CHI'24, UIST'24 <br>
						Conference Review: CHI'19 - '23, UIST'19 - '23, ISS'20, CSCW'21, TEI'20 - '21, MobileHCI'22 <br>
						Journal Review: IMWUT'22, Natural Communications'23
					</p>

					<h4>Awards</h4>
					
					<p style="font-size: 13px">
						First Year Assistant Professor (FYAP) grant, FSU<br>	
					</p>
					
					<!-- <br>
	                
	 				<a class="twitter-timeline" data-width="400" data-height="600" data-theme="light" href="https://twitter.com/TeYenWu2?ref_src=twsrc%5Etfw">Tweets by TeYenWu</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
	                <br> -->
<!-- 					<h4>Press</h4>
	                <ul class="news" style="font-size: 13px">
						
						<li>NSF News: <a href="https://beta.nsf.gov/news/fitbit-face-can-turn-any-face-mask-smart-monitoring-device">'Fitbit for the face' can turn any face mask into smart monitoring device</a></li>
						
	                </ul> -->
	            </div>
            <!-- /right column -->
        </div>
        <footer>
	    <div class="container">
			<small> © 2015 - 2024 All rights reserved.</small>
		</div>
        
</footer>

	</body>
</html>
